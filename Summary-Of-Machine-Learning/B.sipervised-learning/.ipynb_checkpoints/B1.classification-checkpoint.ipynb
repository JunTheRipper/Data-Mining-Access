{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 监督学习专题 —— 分类\n",
    "\n",
    "## 监督学习简介\n",
    "\n",
    "监督学习的目标与特点：\n",
    "\n",
    "利用一组带有标签的数据，学习从输入到输出的映射，然后将这种映射关系应用到未知数据上，达到分类或回归的目的。\n",
    "\n",
    "**分类：当输出是离散的，学习任务为分类任务。**\n",
    "\n",
    "**回归：当输出是连续的，学习任务为回归任务。**\n",
    "\n",
    "### In/Output\n",
    "\n",
    "输入：一组有标签的训练数据(也称观察和评估)，标签表明了这些数\n",
    "据（观察）的所署类别。\n",
    "\n",
    "输出：分类模型根据这些训练数据，训练自己的模型参数，\n",
    "学习出一个适合这组数据的分类器，\n",
    "当有新数据（非训练数据）需要进行类别判断，\n",
    "就可以将这组新数据作为输入送给学好的分类器进行判断。\n",
    "\n",
    "### 评价\n",
    "\n",
    "训练集和测试集的设定\n",
    "\n",
    "训练集(training set):顾名思义用来训练模型的已标注数据，\n",
    "用来建立模型，发现规律。\n",
    "\n",
    "测试集(testing set):也是已标注数据，通常做法是将标注隐藏，\n",
    "输送给训练好的模型，通过结果与真实标注进行对比，评估模型的学习能力。\n",
    "\n",
    "训练集/测试集的划分方法：根据已有标注数据，随机选出一部分数据（70%）数据作为训练数据，余下的作为测试数据，\n",
    "\n",
    "此外还有交叉验证法，自助法用来评估分类模型。\n",
    "\n",
    "#### 评价标准\n",
    "\n",
    "**精确率：**\n",
    "\n",
    "精确率是针对我们预测结果而言的，\n",
    "（以二分类为例）它表示的是预测为正的样本中有多少是真正的正样本。\n",
    "那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，\n",
    "另一种就是把负类预测为正类(FP)，\n",
    "也就是\n",
    "![](Pic/accurency.JPG)\n",
    "\n",
    "**召回率：**\n",
    "\n",
    "是针对我们原来的样本而言的，\n",
    "它表示的是样本中的正例有多少被预测正确了。\n",
    "那也有两种可能，一种是把原来的正类预测成正类(TP)，\n",
    "另一种就是把原来的正类预测为负类(FN)，也就是\n",
    "![](Pic/back.JPG)\n",
    "\n",
    "假设我们手上有60个正样本，40个负样本，\n",
    "我们要找出所有的正样本，\n",
    "分类算法查找出50个，\n",
    "其中只有40个是真正的正样本，\n",
    "\n",
    "TP: 将正类预测为正类数 40；\n",
    "\n",
    "FN: 将正类预测为负类数 20；\n",
    "\n",
    "FP: 将负类预测为正类数 10；\n",
    "\n",
    "TN: 将负类预测为负类数 30\n",
    "\n",
    "准确率（accuracy）=预测对的/所有 = (TP+TN)/(TP+FN+FP+TN) = 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn 中的分类\n",
    "\n",
    "与聚类算法被统一封装在sklearn.cluster模块不同，\n",
    "\n",
    "sklearn库中的分类算法并未被统一封装在一个子模块中，\n",
    "\n",
    "因此对分类算法的import方式各有不同。\n",
    "\n",
    "Sklearn提供的分类函数包括：\n",
    "\n",
    "    • k近邻（knn）\n",
    "    • 朴素贝叶斯（naivebayes），\n",
    "    • 支持向量机（svm），\n",
    "    • 决策树 （decision tree）\n",
    "    • 神经网络模型（Neural networks）等\n",
    "    • 这其中有线性分类器，也有非线性分类器。\n",
    "\n",
    "大致的图像（线性和非线性）的对比\n",
    "\n",
    "![](Pic/classDif.JPG)\n",
    "\n",
    "#### 分类算法应用\n",
    "\n",
    "金融：贷款是否批准进行评估\n",
    "\n",
    " 医疗诊断：判断一个肿瘤是恶性还是良性\n",
    "\n",
    " 欺诈检测：判断一笔银行的交易是否涉嫌欺诈\n",
    "\n",
    " 网页分类：判断网页的所属类别，财经或者是娱乐\n",
    "\n",
    "## 实验数据\n",
    "\n",
    "### Heart Attack Analysis & Prediction Dataset\n",
    "\n",
    "A dataset for heart attack classification\n",
    "\n",
    "心脏病发作分析和预测数据集\n",
    "\n",
    "#### 数据集介绍\n",
    "\n",
    "数据来源： kaggle\n",
    "\n",
    "url: [https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset](https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset)\n",
    "\n",
    "数据条目：302条\n",
    "\n",
    "年龄：病人的年龄    Age : Age of the patient\n",
    "\n",
    "性别：患者的性别    Sex : Sex of the patient\n",
    "\n",
    "exang：运动引起的心绞痛（1=是；0=否）    exang: exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "ca：主要血管数量（0-3） ca: number of major vessels (0-3)\n",
    "\n",
    "cp：胸痛型胸痛型 cp : Chest Pain type chest pain type\n",
    "\n",
    "Value 1：典型的心绞痛   Value 1: typical angina\n",
    "\n",
    "Value 2：非典型心绞痛   Value 2: atypical angina\n",
    "\n",
    "Value 3：非心绞痛      Value 3: non-anginal pain\n",
    "\n",
    "Value 4：无症状    Value 4: asymptomatic\n",
    "\n",
    "trtbps：静息血压（以mm Hg为单位）    trtbps : resting blood pressure (in mm Hg)\n",
    "\n",
    "chol：通过BMI传感器获取的mg/dl胆固醇  chol : cholestoral in mg/dl fetched via BMI sensor\n",
    "\n",
    "fbs：（空腹血糖>120 mg/dl）（1=真；0=假） fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "\n",
    "rest_ecg：静息心电图结果\n",
    "rest_ecg : resting electrocardiographic results\n",
    "\n",
    "值0：正常    Value 0: normal\n",
    "\n",
    "值1:ST-T波异常（T波倒置和/或ST抬高或降低>0.05 mV）    Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\n",
    "值2：根据Estes的标准显示可能或确定的左心室肥大    Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "\n",
    "thalach：达到最大心率    thalach : maximum heart rate achieved\n",
    "\n",
    "目标：0=心脏病发作的机会减少1=心脏病发作的机会增加\n",
    "\n",
    "Slp:坡度\n",
    "\n",
    "target :(output 维度)\n",
    "\n",
    "0= less chance of heart attack\n",
    "\n",
    "1= more chance of heart attack\n",
    "\n",
    "##### 实验预达到目的\n",
    "\n",
    "需求：假设现在出现了一个新用户，但我们只有传感器采集的数据，如何预测他心脏病发作可能性机会增加还是减少呢？\n",
    "\n",
    "在明确这是一个分类问题的情况下，\n",
    "我们可以选定某种分类模型（或者说是算法），通过使用训练数据进行模型学习，然后对每个测试样本给出对应的分类结果。\n",
    "\n",
    "机器学习的分类算法众多，\n",
    "\n",
    "在接下来的学习中我们将会详细介绍经典的分类算法，\n",
    "\n",
    "如K近邻、决策树和朴素贝叶斯的原理和实现，在此之前，我们先尝试对数据进行预处理\n",
    "\n",
    "### 实验基本流程\n",
    "\n",
    "需要从特征文件和标签文件中将所有数据加载到内存\n",
    "中，如果存在缺失值，此步骤还需要进行简单的数据预处理。\n",
    "\n",
    " 创建对应的分类器，并使用训练数据进行训练。\n",
    "\n",
    " 利用测试集预测，通过使用真实值和预测值的比对，计算模型整体的准确率和召回率，来评测模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
       "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
       "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
       "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
       "5   57    1   0     140   192    0        1       148     0      0.4    1   \n",
       "6   56    0   1     140   294    0        0       153     0      1.3    1   \n",
       "7   44    1   1     120   263    0        1       173     0      0.0    2   \n",
       "8   52    1   2     172   199    1        1       162     0      0.5    2   \n",
       "9   57    1   2     150   168    0        1       174     0      1.6    2   \n",
       "\n",
       "   caa  thall  output  \n",
       "0    0      1       1  \n",
       "1    0      2       1  \n",
       "2    0      2       1  \n",
       "3    0      2       1  \n",
       "4    0      2       1  \n",
       "5    0      1       1  \n",
       "6    0      2       1  \n",
       "7    0      3       1  \n",
       "8    0      3       1  \n",
       "9    0      2       1  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataSet = pd.read_csv('heart.csv')\n",
    "dataSet.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         False\n",
       "sex         False\n",
       "cp          False\n",
       "trtbps      False\n",
       "chol        False\n",
       "fbs         False\n",
       "restecg     False\n",
       "thalachh    False\n",
       "exng        False\n",
       "oldpeak     False\n",
       "slp         False\n",
       "caa         False\n",
       "thall       False\n",
       "output      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 缺失值判断\n",
    "dataSet.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "              age\n",
      "count  303.000000\n",
      "mean    54.366337\n",
      "std      9.082101\n",
      "min     29.000000\n",
      "25%     47.500000\n",
      "50%     55.000000\n",
      "75%     61.000000\n",
      "max     77.000000\n",
      "sex\n",
      "              sex\n",
      "count  303.000000\n",
      "mean     0.683168\n",
      "std      0.466011\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      1.000000\n",
      "75%      1.000000\n",
      "max      1.000000\n",
      "cp\n",
      "               cp\n",
      "count  303.000000\n",
      "mean     0.966997\n",
      "std      1.032052\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      1.000000\n",
      "75%      2.000000\n",
      "max      3.000000\n",
      "trtbps\n",
      "           trtbps\n",
      "count  303.000000\n",
      "mean   131.623762\n",
      "std     17.538143\n",
      "min     94.000000\n",
      "25%    120.000000\n",
      "50%    130.000000\n",
      "75%    140.000000\n",
      "max    200.000000\n",
      "chol\n",
      "             chol\n",
      "count  303.000000\n",
      "mean   246.264026\n",
      "std     51.830751\n",
      "min    126.000000\n",
      "25%    211.000000\n",
      "50%    240.000000\n",
      "75%    274.500000\n",
      "max    564.000000\n",
      "fbs\n",
      "              fbs\n",
      "count  303.000000\n",
      "mean     0.148515\n",
      "std      0.356198\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      0.000000\n",
      "75%      0.000000\n",
      "max      1.000000\n",
      "restecg\n",
      "          restecg\n",
      "count  303.000000\n",
      "mean     0.528053\n",
      "std      0.525860\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      1.000000\n",
      "75%      1.000000\n",
      "max      2.000000\n",
      "thalachh\n",
      "         thalachh\n",
      "count  303.000000\n",
      "mean   149.646865\n",
      "std     22.905161\n",
      "min     71.000000\n",
      "25%    133.500000\n",
      "50%    153.000000\n",
      "75%    166.000000\n",
      "max    202.000000\n",
      "exng\n",
      "             exng\n",
      "count  303.000000\n",
      "mean     0.326733\n",
      "std      0.469794\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      0.000000\n",
      "75%      1.000000\n",
      "max      1.000000\n",
      "oldpeak\n",
      "          oldpeak\n",
      "count  303.000000\n",
      "mean     1.039604\n",
      "std      1.161075\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      0.800000\n",
      "75%      1.600000\n",
      "max      6.200000\n",
      "slp\n",
      "              slp\n",
      "count  303.000000\n",
      "mean     1.399340\n",
      "std      0.616226\n",
      "min      0.000000\n",
      "25%      1.000000\n",
      "50%      1.000000\n",
      "75%      2.000000\n",
      "max      2.000000\n",
      "caa\n",
      "              caa\n",
      "count  303.000000\n",
      "mean     0.729373\n",
      "std      1.022606\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      0.000000\n",
      "75%      1.000000\n",
      "max      4.000000\n",
      "thall\n",
      "            thall\n",
      "count  303.000000\n",
      "mean     2.313531\n",
      "std      0.612277\n",
      "min      0.000000\n",
      "25%      2.000000\n",
      "50%      2.000000\n",
      "75%      3.000000\n",
      "max      3.000000\n",
      "output\n",
      "           output\n",
      "count  303.000000\n",
      "mean     0.544554\n",
      "std      0.498835\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      1.000000\n",
      "75%      1.000000\n",
      "max      1.000000\n"
     ]
    }
   ],
   "source": [
    "for i in dataSet.columns:\n",
    "    print(i)\n",
    "    print(dataSet[i].to_frame().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "注意，做测试的情况要处理顺序问题，即，要打乱顺序.原始数据在target的结果集上是有顺序的，先1后0，需要打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>208</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "280   42    1   0     136   315    0        1       125     1      1.8    1   \n",
       "140   51    0   2     120   295    0        0       157     0      0.6    2   \n",
       "24    40    1   3     140   199    0        1       178     1      1.4    2   \n",
       "137   62    1   1     128   208    1        0       140     0      0.0    2   \n",
       "104   50    1   2     129   196    0        1       163     0      0.0    2   \n",
       "..   ...  ...  ..     ...   ...  ...      ...       ...   ...      ...  ...   \n",
       "149   42    1   2     130   180    0        1       150     0      0.0    2   \n",
       "88    54    0   2     110   214    0        1       158     0      1.6    1   \n",
       "1     37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "122   41    0   2     112   268    0        0       172     1      0.0    2   \n",
       "240   70    1   2     160   269    0        1       112     1      2.9    1   \n",
       "\n",
       "     caa  thall  output  \n",
       "280    0      1       0  \n",
       "140    0      2       1  \n",
       "24     0      3       1  \n",
       "137    0      2       1  \n",
       "104    0      2       1  \n",
       "..   ...    ...     ...  \n",
       "149    0      2       1  \n",
       "88     0      2       1  \n",
       "1      0      2       1  \n",
       "122    0      2       1  \n",
       "240    1      3       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "dataSet = shuffle(dataSet)\n",
    "dataSet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据总体完善，无异常值，可以进行直接选择处理,先分割成训练集和测试集,总共300多条数据，我们选择260条作为训练集，最后40条作为测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataSet_train = dataSet[:260]\n",
    "dataSet_test =  dataSet[260:]\n",
    "\n",
    "# 训练集\n",
    "y_data_train = dataSet_train['output']\n",
    "# print(y_data)\n",
    "x_data_train = dataSet_train.iloc[:,:-1]\n",
    "\n",
    "# 测试集\n",
    "y_data_test = dataSet_test['output']\n",
    "\n",
    "x_data_test = dataSet_test.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 KNN\n",
    "\n",
    "KNN：通过计算待分类数据点，与已有数据集中的所有数据点的距离。\n",
    "\n",
    "取距离最小的前K个点，根据“少数服从多数“的原则，将这个数据点划分为出现次数最多的那个类别。\n",
    "\n",
    "### sklearn 的 KNN\n",
    "\n",
    "**sklearn中的K近邻分类器**\n",
    "\n",
    "在sklearn库中，可以使用sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "创建一个K近邻分类器，主要参数有：\n",
    "\n",
    "    • n_neighbors：用于指定分类器中K的大小(默认值为5，注意与kmeans的区别)\n",
    "\n",
    "    • weights：设置选中的K个点对分类结果影响的权重（默认值为平均权重“uniform”，可以选择“distance”代表越近的点权重越高，或者传入自己编写的以距离为参数的权重计算函数）\n",
    "\n",
    "    • algorithm：设置用于计算临近点的方法，因为当数据量很大的情况下计算当前点和所有点的距离再选出最近的k各点，这个计算量是很费时的，所以（选项中有ball_tree、kd_tree和brute，分别代表不同的寻找邻居的优化算法，默认值为auto，根据训练数据自动选择）\n",
    "\n",
    "#### K近邻分类器的使用\n",
    "\n",
    "例子：\n",
    "I 创建一组数据 X 和它对应的标签 y：\n",
    "\n",
    "x = [[0], [1], [2], [3]]\n",
    "\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "II 使用 import 语句导入 K 近邻分类器\n",
    "\n",
    "III 参数 n_neighbors 设置为 3，即使用最近的3个邻居作为分类的依据，其他参\n",
    "数保持默认值，并将创建好的实例赋给变量 neigh。\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "IV 调用 fit() 函数，将训练数据 X 和 标签 y 送入分类器进行学习。\n",
    "\n",
    "neigh.fit(X, y)\n",
    "\n",
    "V 调用 predict() 函数，对未知分类样本 [1.1] 分类，可以直接并将需要分类\n",
    "的数据构造为数组形式作为参数传入，得到分类标签作为返回值。\n",
    "\n",
    "**print(neigh.predict([[1.1]]))**\n",
    "\n",
    "样例输出值是 0，表示K近邻分类器通过计算样本 [1.1] 与训练数据的距离，\n",
    "取 0,1,2 这 3 个邻居作为依据，根据“投票法”最终将样本分为类别 0。\n",
    "\n",
    "#### KNN 选择经验\n",
    "\n",
    "在实际使用时，我们可以使用所有训练数据构成特征 X 和标签 y，\n",
    "使用fit() 函数进行训练。在正式分类时，\n",
    "通过一次性构造测试集或者一个一个输入样本的方式，得到样本对应的分类结果。\n",
    "\n",
    "有关K的取值：\n",
    "\n",
    "    • 如果较大，相当于使用较大邻域中的训练实例进行预测，可以减小估计误差，但是距离较远的样本也会对预测起作用，导致预测错误。\n",
    "    • 相反地，如果 K 较小，相当于使用较小的邻域进行预测，如果邻居恰好是噪声点，会导致过拟合。\n",
    "    • 一般情况下，K 会倾向选取较小的值，并使用交叉验证法选取最优 K 值\n",
    "下面我们进行实战操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction All Done!\n",
      "\n",
      "\n",
      "The classification report for knn:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72        21\n",
      "           1       0.72      0.82      0.77        22\n",
      "\n",
      "    accuracy                           0.74        43\n",
      "   macro avg       0.75      0.74      0.74        43\n",
      "weighted avg       0.75      0.74      0.74        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report # 结果预测评估模块\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn = knn.fit(x_data_train, y_data_train)\n",
    "\n",
    "answer_knn = knn.predict(x_data_test)\n",
    "print(\"Prediction All Done!\")\n",
    "\n",
    "print('\\n\\nThe classification report for knn:')\n",
    "print(classification_report(y_data_test, answer_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在再来尝试一下标准化参考看一下效果.\n",
    "\n",
    "标准化的目的与归一化一样，都是为了避免某个特征的重要程度过大或过小。\n",
    "\n",
    "标准化的优点：受异常点的影响较小。 适用于繁杂大数据。\n",
    "\n",
    "这里用到了\n",
    "**from sklearn.preprocessing import StandardScaler**方法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction All Done!\n",
      "\n",
      "\n",
      "The classification report for knn:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79        21\n",
      "           1       0.77      0.91      0.83        22\n",
      "\n",
      "    accuracy                           0.81        43\n",
      "   macro avg       0.83      0.81      0.81        43\n",
      "weighted avg       0.82      0.81      0.81        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据标准化\n",
    "from sklearn.preprocessing import StandardScaler#用于对数据进行标准化操作\n",
    "dataSet_xtrain_std = StandardScaler().fit_transform(x_data_train)\n",
    "dataSet_xtest_std = StandardScaler().fit_transform(x_data_test)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report # 结果预测评估模块\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn = knn.fit(dataSet_xtrain_std, y_data_train)\n",
    "\n",
    "answer_knn = knn.predict(dataSet_xtest_std)\n",
    "print(\"Prediction All Done!\")\n",
    "\n",
    "print('\\n\\nThe classification report for knn:')\n",
    "print(classification_report(y_data_test, answer_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很明显，得到的测试集成功率比之前的更高了，当然每次训练得到的数据结果不尽相同，\n",
    "\n",
    "这是因为：\n",
    "\n",
    "初试位置选择不同算法收敛到不同结果。\n",
    "\n",
    "我们尝试使用knn参数 n_neighbors，即用于指定分类器中K的大小调整再查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction All Done!\n",
      "\n",
      "\n",
      "The classification report for knn:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82        21\n",
      "           1       0.80      0.91      0.85        22\n",
      "\n",
      "    accuracy                           0.84        43\n",
      "   macro avg       0.84      0.84      0.84        43\n",
      "weighted avg       0.84      0.84      0.84        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(6)\n",
    "\n",
    "knn = knn.fit(dataSet_xtrain_std, y_data_train)\n",
    "\n",
    "answer_knn = knn.predict(dataSet_xtest_std)\n",
    "print(\"Prediction All Done!\")\n",
    "\n",
    "print('\\n\\nThe classification report for knn:')\n",
    "print(classification_report(y_data_test, answer_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN 优缺点：\n",
    "\n",
    "优点：\n",
    "\n",
    "    ① 简单，易于理解，易于实现，无需参数估计，无需训练;\n",
    "    ② 对异常值不敏感（个别噪音数据对结果的影响不是很大）;\n",
    "    ③ 适合对稀有事件进行分类;\n",
    "    ④ 适合于多分类问题(multi-modal,对象具有多个类别标签)，KNN要比SVM表现要好;\n",
    "\n",
    "缺点：\n",
    "\n",
    "    ① 对测试样本分类时的计算量大，内存开销大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本; \n",
    "    ② 可解释性差，无法告诉你哪个变量更重要，无法给出决策树那样的规则；\n",
    "    ③ K值的选择：最大的缺点是当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进; \n",
    "    ④ KNN是一种消极学习方法、懒惰算法。\n",
    "\n",
    "## 2 决策树\n",
    "\n",
    "决策树是一种树形结构的分类器，\n",
    "通过顺序询问分类点的属性决定分类点最终的类别。\n",
    "\n",
    "通常根据特征的信息增益或其他指标，构建一颗决策树。\n",
    "\n",
    "在分类时，只需要按照决策树中的结点依次进行判断，即可得到样本所属类别。\n",
    "\n",
    "例如，根据下图这个构造好的分类决策树，\n",
    "一个无房产，单身，年收入55K的人的会被归入无法偿还信用卡这个类别。\n",
    "\n",
    "![](Pic/Dis.JPG)\n",
    "\n",
    "### sklearn 中的决策树\n",
    "\n",
    "在sklearn库中，可以使用**sklearn.tree.DecisionTreeClassifier**创建一个决策树用于分类，其主要参数有：\n",
    "\n",
    "    criterion ：用于选择属性的准则，可以传入“gini”代表基尼系数，或者“entropy”代表信息增益。\n",
    "    max_features ：表示在决策树结点进行分裂时，从多少个特征中选择最优特征。可以设定固定数目、百分比或其他标准。它的默认值是使用所有特征个数。\n",
    "\n",
    "下面使用简单示例（鸢尾花数据）展现相关决策树的分类状态，并尝试交叉验证技术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # 鸢尾花数据集\n",
    "from sklearn.tree import DecisionTreeClassifier # 决策树分类器\n",
    "from sklearn.model_selection import cross_val_score # 交叉验证数据集的技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面进行决策树的使用.\n",
    "\n",
    "这里我们将决策树分类器做为待评估的模型，\n",
    "iris.data鸢尾花数据做为特征，\n",
    "\n",
    "iris.target鸢尾花分类标签做为目标结果，通过设定cv为10，使用10折交叉验证。得到最终的交叉验证得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.93333333, 1.        , 0.93333333, 0.93333333,\n",
       "       0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "irs = load_iris()\n",
    "k = cross_val_score(clf, irs.data, irs.target, cv=10)\n",
    "# clf.fit()\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "以仿照之前 K近邻分类器的使用方法，利用 fit() 函数训练模型并使用predict() 函数预测：\n",
    "\n",
    "决策树本质上是寻找一种对特征空间上的划分，旨在构建一个训练数据拟合的好，并且复杂度小的决策树。\n",
    "\n",
    "• 在实际使用中，需要根据数据情况，调整DecisionTreeClassifier类中传入的参数，比如选择合适的criterion，设置随机变量等。\n",
    "\n",
    "下面我们尝试对上述的心脏病数据进行决策树分类挖掘，同上，我们对标准化和非标准化进行分开分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction All Done!\n",
      "\n",
      "\n",
      "The classification report for DecisionTree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76        21\n",
      "           1       0.77      0.77      0.77        22\n",
      "\n",
      "    accuracy                           0.77        43\n",
      "   macro avg       0.77      0.77      0.77        43\n",
      "weighted avg       0.77      0.77      0.77        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 非标准化\n",
    "from sklearn.tree import DecisionTreeClassifier # 决策树分类器\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "dtree = dtree.fit(x_data_train, y_data_train)\n",
    "\n",
    "answer_dtree = dtree.predict(x_data_test)\n",
    "print(\"Prediction All Done!\")\n",
    "\n",
    "print('\\n\\nThe classification report for DecisionTree:')\n",
    "print(classification_report(y_data_test, answer_dtree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同上，进行标准化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction All Done!\n",
      "\n",
      "\n",
      "The classification report for DecisionTree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86        21\n",
      "           1       0.90      0.82      0.86        22\n",
      "\n",
      "    accuracy                           0.86        43\n",
      "   macro avg       0.86      0.86      0.86        43\n",
      "weighted avg       0.86      0.86      0.86        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 标准化后的结果\n",
    "dtree = dtree.fit(dataSet_xtrain_std, y_data_train)\n",
    "\n",
    "answer_dtree = dtree.predict(dataSet_xtest_std)\n",
    "print(\"Prediction All Done!\")\n",
    "\n",
    "print('\\n\\nThe classification report for DecisionTree:')\n",
    "print(classification_report(y_data_test, answer_dtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树是一种典型的分类方法\n",
    "\n",
    "    1.首先对数据进行处理，利用归纳算法生成可读的规则和决策树\n",
    "    2.然后使用决策对新数据进行分析\n",
    "本质上决策树是通过一系列规则对数据进行分类的过程\n",
    "\n",
    "### 决策树算法的优点\n",
    "1、推理过程容易理解，决策推理过程可以表示为If — Then 形式；\n",
    "2、推理过程完全依赖于属性变量的取值特点；\n",
    "3、可自动忽略目标变量没有贡献的属性变量，也为判断属性变量的重要性，减少变量的数据提供参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 朴素贝叶斯\n",
    "\n",
    "朴素贝叶斯分类器是一个以贝叶斯定理为基础的多分类的分类器。\n",
    "\n",
    "对于给定数据，首先基于特征的条件独立性假设，学习输入输出的联合概率分布，\n",
    "\n",
    "然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。\n",
    "\n",
    "![](Pic/bays.JPG)\n",
    "\n",
    "### sklearn 中的朴素贝叶斯\n",
    "\n",
    "在sklearn库中，实现了三个朴素贝叶斯分类器，如下表所示\n",
    "\n",
    "![](Pic/baysTB.JPG)\n",
    "\n",
    "在sklearn库中，可以使用sklearn.naive_bayes.GaussianNB创建一个高斯朴素贝叶斯分类器，\n",
    "\n",
    "其参数有：\n",
    "\n",
    "• priors ：给定各个类别的先验概率。如果为空，则按训练数据的实际情况进行统计；如果给定先验概率，则在训练过程中不能更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  # 高斯朴素贝叶斯分类器\n",
    "import numpy as np\n",
    "X =  np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y =  np.array([1, 1, 1, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用默认参数，创建一个高斯朴素贝叶斯分类器，并将该分类器赋给变量clf。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = GaussianNB(priors=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似的，使用 fit() 函数进行训练，并使用 predict() 函数进行预测，得到\n",
    "预测结果为 1。（测试时可以构造二维数组达到同时预测多个样本的目的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X,Y)\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 总结：\n",
    "朴素贝叶斯是典型的生成学习方法，由训练数据学习联合概率分布，并求得后验概率分布。\n",
    "\n",
    "朴素贝叶斯一般在小规模数据上的表现很好，适合进行多分类任务\n",
    "\n",
    "下面我们尝试对上述的心脏病数据进行朴素贝叶斯分类挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction All Done!\n",
      "\n",
      "\n",
      "The classification report for GaussianNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66        21\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.49        43\n",
      "   macro avg       0.24      0.50      0.33        43\n",
      "weighted avg       0.24      0.49      0.32        43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  # 高斯朴素贝叶斯分类器\n",
    "gNB = GaussianNB()\n",
    "\n",
    "gNB = gNB.fit(x_data_train, y_data_train)\n",
    "\n",
    "answer_gNB = dtree.predict(x_data_test)\n",
    "print(\"Prediction All Done!\")\n",
    "\n",
    "print('\\n\\nThe classification report for GaussianNB:')\n",
    "print(classification_report(y_data_test, answer_gNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction All Done!\n",
      "\n",
      "\n",
      "The classification report for GaussianNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66        21\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.49        43\n",
      "   macro avg       0.24      0.50      0.33        43\n",
      "weighted avg       0.24      0.49      0.32        43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB  # 高斯朴素贝叶斯分类器\n",
    "gNB = BernoulliNB()\n",
    "\n",
    "gNB = gNB.fit(x_data_train, y_data_train)\n",
    "\n",
    "answer_gNB = dtree.predict(x_data_test)\n",
    "print(\"Prediction All Done!\")\n",
    "\n",
    "print('\\n\\nThe classification report for GaussianNB:')\n",
    "print(classification_report(y_data_test, answer_gNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "同样，我们尝试对文本进行先标准化再去做机器学习，查看相关的数据模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction All Done!\n",
      "\n",
      "\n",
      "The classification report for DecisionTree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78        21\n",
      "           1       0.78      0.82      0.80        22\n",
      "\n",
      "    accuracy                           0.79        43\n",
      "   macro avg       0.79      0.79      0.79        43\n",
      "weighted avg       0.79      0.79      0.79        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gNB = gNB.fit(dataSet_xtrain_std, y_data_train)\n",
    "\n",
    "answer_gNB = gNB.predict(dataSet_xtest_std)\n",
    "print(\"Prediction All Done!\")\n",
    "\n",
    "print('\\n\\nThe classification report for DecisionTree:')\n",
    "print(classification_report(y_data_test, answer_gNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "接下来我们尝试对数据集进行交叉验证技术再进行操作,分别做KNN、决策树、贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  15  16  17  19  20\n",
      "  21  22  23  24  27  28  29  30  31  32  33  34  35  36  37  38  40  41\n",
      "  43  44  45  46  47  48  49  50  51  54  55  56  57  58  59  60  61  62\n",
      "  63  64  65  66  67  68  69  70  71  72  73  74  75  77  78  79  80  81\n",
      "  82  83  84  85  86  87  89  90  91  92  94  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 110 111 112 114 115 116 117 118 119 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 135 136 138 139 140 141 142 143 144\n",
      " 145 147 148 149 151 152 153 154 155 156 157 158 159 160 162 163 164 165\n",
      " 167 168 169 171 172 173 174 175 176 177 178 179 180 182 183 184 185 186\n",
      " 187 188 189 190 192 193 194 195 196 197 199 200 201 202 203 204 206 207\n",
      " 208 209 210 211 212 213 214 216 217 218 219 220 221 222 223 224 225 226\n",
      " 228 230 231 232 233 234 236 238 239 240 241 242 243 244 246 247 248 249\n",
      " 250 251 252 253 254 255 256 257 258 259 260 263 264 266 267 268 269 270\n",
      " 271 272 273 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289\n",
      " 290 291 292 293 294 295 296 297 298 299 300 301 302] \n",
      "TEST: [  8  14  18  25  26  39  42  52  53  76  88  93  95  96 109 113 120 134\n",
      " 137 146 150 161 166 170 181 191 198 205 215 227 229 235 237 245 261 262\n",
      " 265 274]\n",
      "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  24  25  26  27  28  29  30  31  32  33  34  36  37  38\n",
      "  39  40  41  42  43  45  46  47  48  49  51  52  53  54  55  56  58  59\n",
      "  60  61  62  63  64  65  66  67  70  71  73  74  75  76  77  78  79  80\n",
      "  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
      " 101 102 103 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
      " 121 122 123 124 125 126 129 130 131 132 133 134 135 136 137 139 140 144\n",
      " 145 146 148 149 150 151 152 154 155 157 158 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 179 180 181 182 183 185 186\n",
      " 187 188 189 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205\n",
      " 206 207 208 209 210 212 213 214 215 217 218 221 222 223 224 225 226 227\n",
      " 228 229 230 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246\n",
      " 247 248 249 250 251 252 253 255 256 257 258 259 260 261 262 263 265 266\n",
      " 267 268 269 270 271 272 273 274 276 278 279 280 282 283 284 285 286 287\n",
      " 289 290 291 292 293 294 295 296 297 298 299 301 302] \n",
      "TEST: [  2  23  35  44  50  57  68  69  72  83 100 104 119 127 128 138 141 142\n",
      " 143 147 153 156 159 178 184 190 211 216 219 220 231 254 264 275 277 281\n",
      " 288 300]\n",
      "TRAIN: [  0   1   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  47  48  49  50  51  52  53  55  56  57\n",
      "  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75\n",
      "  76  77  79  80  81  82  83  84  86  87  88  89  91  92  93  94  95  96\n",
      "  98  99 100 101 102 103 104 106 107 108 109 111 112 113 114 115 116 117\n",
      " 118 119 120 121 122 123 125 126 127 128 129 130 131 133 134 135 136 137\n",
      " 138 139 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156\n",
      " 157 158 159 160 161 162 163 164 165 166 167 168 170 172 173 174 177 178\n",
      " 179 180 181 182 183 184 185 186 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 201 202 204 205 207 208 209 210 211 213 214 215 216 217 219 220\n",
      " 222 224 225 226 227 229 231 232 234 235 236 237 238 239 240 243 245 246\n",
      " 248 249 251 252 254 255 256 257 258 259 260 261 262 263 264 265 266 267\n",
      " 268 269 270 271 272 274 275 276 277 278 280 281 282 283 285 286 288 289\n",
      " 290 291 292 293 294 295 296 297 298 299 300 301 302] \n",
      "TEST: [  4  28  46  54  78  85  90  97 105 110 124 132 140 169 171 175 176 187\n",
      " 200 203 206 212 218 221 223 228 230 233 241 242 244 247 250 253 273 279\n",
      " 284 287]\n",
      "TRAIN: [  0   2   3   4   6   7   8   9  11  13  14  15  16  17  18  19  20  22\n",
      "  23  25  26  27  28  29  30  31  32  34  35  36  37  38  39  41  42  43\n",
      "  44  45  46  47  48  49  50  51  52  53  54  55  57  58  59  61  62  63\n",
      "  64  65  66  67  68  69  70  72  73  75  76  77  78  79  81  83  85  86\n",
      "  87  88  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 127 128 130 131 132 133 134 135 136 137 138 140 141 142 143 145 146\n",
      " 147 150 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168\n",
      " 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186\n",
      " 187 188 189 190 191 192 193 194 196 197 198 200 202 203 204 205 206 207\n",
      " 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225\n",
      " 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 244\n",
      " 245 246 247 249 250 253 254 255 256 258 259 260 261 262 263 264 265 266\n",
      " 267 268 269 271 272 273 274 275 276 277 279 280 281 282 283 284 285 286\n",
      " 287 288 289 290 292 294 295 297 298 299 300 301 302] \n",
      "TEST: [  1   5  10  12  21  24  33  40  56  60  71  74  80  82  84  89 106 126\n",
      " 129 139 144 148 149 151 152 195 199 201 243 248 251 252 257 270 278 291\n",
      " 293 296]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  37  38  39  40  41  42  43  44  46  47  49  50  51  52  53  54  56  57\n",
      "  58  59  60  61  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
      "  80  81  82  83  84  85  87  88  89  90  91  93  94  95  96  97  98  99\n",
      " 100 101 102 104 105 106 107 108 109 110 111 112 113 115 116 117 119 120\n",
      " 121 122 124 125 126 127 128 129 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 146 147 148 149 150 151 152 153 154 155 156 157 159 161\n",
      " 164 165 166 167 168 169 170 171 174 175 176 177 178 179 180 181 182 184\n",
      " 185 186 187 188 190 191 193 194 195 196 197 198 199 200 201 203 204 205\n",
      " 206 207 209 210 211 212 214 215 216 217 218 219 220 221 222 223 224 227\n",
      " 228 229 230 231 233 234 235 236 237 238 239 241 242 243 244 245 246 247\n",
      " 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265\n",
      " 266 267 268 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284\n",
      " 285 286 287 288 290 291 293 296 297 298 299 300 302] \n",
      "TEST: [ 36  45  48  55  62  64  67  79  86  92 103 114 118 123 130 145 158 160\n",
      " 162 163 172 173 183 189 192 202 208 213 225 226 232 240 269 289 292 294\n",
      " 295 301]\n",
      "TRAIN: [  0   1   2   3   4   5   7   8   9  10  12  13  14  15  17  18  20  21\n",
      "  23  24  25  26  27  28  29  30  32  33  34  35  36  38  39  40  42  43\n",
      "  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59  60  62  64\n",
      "  66  67  68  69  70  71  72  74  76  77  78  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 118 119 120 122 123 124 125\n",
      " 126 127 128 129 130 132 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 149 150 151 152 153 156 157 158 159 160 161 162 163 164 165\n",
      " 166 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n",
      " 186 187 189 190 191 192 193 195 196 198 199 200 201 202 203 204 205 206\n",
      " 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224\n",
      " 225 226 227 228 229 230 231 232 233 235 236 237 238 239 240 241 242 243\n",
      " 244 245 247 248 250 251 252 253 254 255 256 257 259 260 261 262 264 265\n",
      " 266 267 269 270 271 272 273 274 275 277 278 279 281 282 284 285 286 287\n",
      " 288 289 291 292 293 294 295 296 298 299 300 301 302] \n",
      "TEST: [  6  11  16  19  22  31  37  41  51  61  63  65  73  75  98 116 117 121\n",
      " 131 133 154 155 167 168 188 194 197 234 246 249 258 263 268 276 280 283\n",
      " 290 297]\n",
      "TRAIN: [  0   1   2   4   5   6   8  10  11  12  13  14  16  17  18  19  21  22\n",
      "  23  24  25  26  28  29  31  32  33  34  35  36  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  71  72  73  74  75  76  78  79  80\n",
      "  81  82  83  84  85  86  88  89  90  91  92  93  95  96  97  98 100 103\n",
      " 104 105 106 109 110 112 113 114 116 117 118 119 120 121 123 124 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 178 181 183 184 185 186\n",
      " 187 188 189 190 191 192 194 195 197 198 199 200 201 202 203 204 205 206\n",
      " 208 209 210 211 212 213 215 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244\n",
      " 245 246 247 248 249 250 251 252 253 254 257 258 259 261 262 263 264 265\n",
      " 266 267 268 269 270 273 274 275 276 277 278 279 280 281 283 284 285 286\n",
      " 287 288 289 290 291 292 293 294 295 296 297 300 301] \n",
      "TEST: [  3   7   9  15  20  27  30  70  77  87  94  99 101 102 107 108 111 115\n",
      " 122 125 164 177 179 180 182 193 196 207 214 255 256 260 271 272 282 298\n",
      " 299 302]\n",
      "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  18  19  20\n",
      "  21  22  23  24  25  26  27  28  30  31  33  35  36  37  39  40  41  42\n",
      "  44  45  46  48  50  51  52  53  54  55  56  57  60  61  62  63  64  65\n",
      "  67  68  69  70  71  72  73  74  75  76  77  78  79  80  82  83  84  85\n",
      "  86  87  88  89  90  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 113 114 115 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 158 159 160 161 162\n",
      " 163 164 166 167 168 169 170 171 172 173 175 176 177 178 179 180 181 182\n",
      " 183 184 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n",
      " 203 205 206 207 208 211 212 213 214 215 216 218 219 220 221 223 225 226\n",
      " 227 228 229 230 231 232 233 234 235 237 240 241 242 243 244 245 246 247\n",
      " 248 249 250 251 252 253 254 255 256 257 258 260 261 262 263 264 265 268\n",
      " 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302] \n",
      "TEST: [  0  13  17  29  32  34  38  43  47  49  58  59  66  81  91 112 135 136\n",
      " 157 165 174 185 186 204 209 210 217 222 224 236 238 239 259 266 267 285\n",
      " 286]\n",
      "Start training knn\n",
      "Training done\n",
      "Prediction done\n",
      "Start training DT\n",
      "Training done\n",
      "Prediction done\n",
      "Start training Bayes\n",
      "Training done\n",
      "Prediction done\n",
      "\n",
      "\n",
      "The classification report for knn:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.33      0.36        18\n",
      "           1       0.45      0.53      0.49        19\n",
      "\n",
      "    accuracy                           0.43        37\n",
      "   macro avg       0.43      0.43      0.43        37\n",
      "weighted avg       0.43      0.43      0.43        37\n",
      "\n",
      "\n",
      "\n",
      "The classification report for DT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.33      0.41        18\n",
      "           1       0.54      0.74      0.62        19\n",
      "\n",
      "    accuracy                           0.54        37\n",
      "   macro avg       0.54      0.54      0.52        37\n",
      "weighted avg       0.54      0.54      0.52        37\n",
      "\n",
      "\n",
      "\n",
      "The classification report for Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60        18\n",
      "           1       0.64      0.84      0.73        19\n",
      "\n",
      "    accuracy                           0.68        37\n",
      "   macro avg       0.70      0.67      0.66        37\n",
      "weighted avg       0.69      0.68      0.67        37\n",
      "\n",
      "KNN精确度： 0.43243243243243246\n",
      "决策树精确度： 0.5405405405405406\n",
      "高斯贝叶斯精确度： 0.6756756756756757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler # 用于对数据进行标准化操作\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=8,shuffle=True, random_state=2021)\n",
    "y = dataSet['output']\n",
    "# print(y_data)\n",
    "x = dataSet.iloc[:,:-1]\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    print('TRAIN:', train_index, \"\\nTEST:\", test_index)\n",
    "#     .iloc[train_index,:]\n",
    "    X_train, X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print('Start training knn')\n",
    "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "print('Training done')\n",
    "answer_knn = knn.predict(X_test)\n",
    "print('Prediction done')\n",
    "\n",
    "print('Start training DT')\n",
    "dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print('Training done')\n",
    "answer_dt = dt.predict(X_test)\n",
    "print('Prediction done')\n",
    "\n",
    "print('Start training Bayes')\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "print('Training done')\n",
    "answer_gnb = gnb.predict(X_test)\n",
    "print('Prediction done')\n",
    "\n",
    "print('\\n\\nThe classification report for knn:')\n",
    "print(classification_report(y_test, answer_knn))\n",
    "print('\\n\\nThe classification report for DT:')\n",
    "print(classification_report(y_test, answer_dt))\n",
    "print('\\n\\nThe classification report for Bayes:')\n",
    "print(classification_report(y_test, answer_gnb))\n",
    "\n",
    "KNNscore = knn.score(X_test, y_test)\n",
    "\n",
    "print(\"KNN精确度：\",KNNscore)\n",
    "Tscore = dt.score(X_test, y_test)\n",
    "\n",
    "print(\"决策树精确度：\",Tscore)\n",
    "GSscore = gnb.score(X_test, y_test)\n",
    "print(\"高斯贝叶斯精确度：\",GSscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次的数据是两类的划分相关，主要还是不适合贝叶斯分析的分类技术，总体相对分类的结果也是不确定，\n",
    "可能准确度会低于前两种。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
